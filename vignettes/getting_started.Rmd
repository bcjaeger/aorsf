---
title: "Getting started"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting started}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.height = 5, 
  fig.width = 7
)
```

This vignette covers core features of the `aorsf` package. A separate vignette covers each feature in more detail. 

1. For variable importance, see `variable_importance.Rmd`
1. For two-way interactions, see `variable_interaction.Rmd`
1. For partial dependence, see `partial_dependence.Rmd`

# Background: Oblique RSF

The oblique random survival forest (RSF) is an extension of the axis-based RSF algorithm. Numerous R packages offer axis-based splits for survival decision trees, including `RandomForestSRC`, `party`, and `ranger`. 


## What is an oblique decision tree?

Decision trees are developed by splitting a set of training data into two new subsets, with the goal of having more similarity within the new subsets than between them. This splitting process is repeated on the resulting subsets of data until a stopping criterion is met. When the new subsets of data are formed based on a single predictor, the decision tree is said to be axis-based because the splits of the data appear perpendicular to the axis of the predictor. When linear combinations of variables are used instead of a single variable, the tree is oblique because the splits of the data are neither parallel nor at a right angle to the axis. 

```{r fig_oblique_v_axis, out.width='100%', echo = FALSE, fig.cap="Decision trees for classification with axis-based splitting (left) and oblique splitting (right). Cases are orange squares; controls are purple circles. Both trees partition the predictor space defined by variables X1 and X2, but the oblique splits do a better job of separating the two classes."}

knitr::include_graphics('tree_axis_v_oblique.png')

```


# Accelerated ORSF

The purpose of `aorsf` is to provide routines to fit oblique RSFs that will scale adequately to large data sets. For example, `aorsf::orsf()` runs about 500 times faster than its predecessor, `obliqueRSF::ORSF()`.

To fit an accelerated ORSF model, use the `orsf` function:

```{r}
library(aorsf)
library(ggplot2)

set.seed(329)

orsf_fit <- orsf(data_train = pbc_orsf, 
                 formula = Surv(time, status) ~ . - id, 
                 n_tree = 2500)

orsf_fit

```

you may notice that the first input of `aorsf` is `data_train`. This is a design choice that makes it easier to use `orsf` with pipes (i.e., `%>%` or `|>`). For instance,

```{r, eval=FALSE}
library(dplyr)

orsf_fit <- pbc_orsf |> 
 select(-id) |> 
 orsf(formula = Surv(time, status) ~ .)

```

# Variable importance

The coefficients that are used in linear combinations of input variables are a unique characteristic of ORSF that can be used for model interpretation.

For example, to estimate the importance of a variable, ORSF multiplies each coefficient of that variable by -1 and then re-computes the out-of-sample (sometimes referred to as out-of-bag) accuracy of the ORSF model. 

```{r}

variable_importance <- orsf_vi(orsf_fit)

variable_importance

```


# Variable interaction

Another application of linear combination coefficients is measuring a two-way interaction score. ORSF's two-way interaction score for a pair of predictors is the proportion of variability in the coefficient of one predictor explained by the mean of the other. So if one predictor's coefficient is highly correlated with another predictor's mean value, the two-way interaction score will be high (maximum value of the score is 1). If there is no correlation between the coefficient of one predictor and the mean value of another, the two-way interaction score will be low (minimum value of the score is 0).

```{r}
variable_interaction <- orsf_interaction(orsf_fit)

variable_interaction[1:5, ]

```


# Partial dependence

The `orsf_interaction()` function indicated that `ascites` and `bili` have a strong interaction score, and the `orsf_vi()` function indicated that `bili` is the most important predictor. We can use the `orsf_pd()` function (pd stands for partial dependence) to explore how these variables influence predicted risk from ORSF.

We'll use `orsf_pd_summary` below to compute the expected predicted risk for a range of `bili` values when `ascites` is either 0 or 1.

```{r}

pd_spec <- list(ascites = c("0","1"),
                bili = seq(0.6, 7.1, by = 0.1))

pd_data <-
 orsf_pd_summary(object = orsf_fit,
                 pd_spec = pd_spec,
                 times = 1000)

ggplot(pd_data) +
 aes(x = bili, y = mean, col = ascites) +
 geom_line() + 
 labs(y = 'Predicted risk',
      x = 'Bilirubin',
      title = 'Partial dependence of bilirubin and ascites')

```


The presence of ascites clearly has a large effect on predicted risk, which makes it hard to see the interaction effect in this plot. We'll align the partial dependence values for both ascites groups so that both curves will start at a prediction of 0 

```{r}
# aligning predictions at lowest value of bili
min_asc_0 <- with(pd_data, mean[ascites == 0 & bili == 0.6])
min_asc_1 <- with(pd_data, mean[ascites == 1 & bili == 0.6])

pd_data_aligned <-
 within(pd_data, {
  mean[ascites == 0] <- mean[ascites == 0] - min_asc_0
  mean[ascites == 1] <- mean[ascites == 1] - min_asc_1
 })

head(pd_data_aligned)

```

With the aligned partial dependence values we can see the interaction effect clearly. Increasing bilirubin solicits a greater increase in predicted risk for patients who do not have ascites versus patients who do. This is fairly intuitive as increased bilirubin may be a sign of undetected ascites.

```{r}
ggplot(pd_data_aligned) +
 aes(x = bili, y = mean, col = ascites) +
 geom_line() + 
 labs(y = 'predicted risk, centered at Bilirubin = 0.6',
      x = 'Bilirubin',
      title = 'Interaction between bilirubin and ascites')

```

# What about the old ORSF?

The old ORSF (i.e., `obliqueRSF`) used `glmnet` to find linear combinations of inputs. `aorsf` allows users to implement this approach using the `orsf_control_net()` function: 

```{r}

orsf_net <- orsf(data_train = pbc_orsf, 
                 formula = Surv(time, status) ~ . - id, 
                 control = orsf_control_net(),
                 n_tree = 50)

```

Note that the `net` approach is a fair bit slower than the `cph` one!

```{r}

# tracking how long it takes to fit 50 glmnet trees
print(
 system.time(
  orsf(data_train = pbc_orsf, 
       formula = Surv(time, status) ~ . - id, 
       control = orsf_control_net(),
       n_tree = 50)
 )
)

# and how long it takes to fit 50 cph trees
print(
 system.time(
  orsf(data_train = pbc_orsf, 
       formula = Surv(time, status) ~ . - id, 
       control = orsf_control_cph(),
       n_tree = 50)
 )
)


```



